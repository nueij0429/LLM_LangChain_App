{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af9b8121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0693468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ee98644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"ë‹¹ì‹ ì€ ê°œë°œìì…ë‹ˆë‹¤.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”? ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8a270f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001E858703290> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E858614A70> root_client=<openai.OpenAI object at 0x000001E858729310> root_async_client=<openai.AsyncOpenAI object at 0x000001E858700500> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq APIë¥¼ ì‚¬ìš©í•˜ëŠ” ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e09ca344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‘ë‹µ: íŒŒì´ì¬! í”„ë¡œê·¸ë˜ë° ì„¸ê³„ì—ì„œ ë§¤ìš° ì¸ê¸° ìˆëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. íŒŒì´ì¬ì€ 1991ë…„ì— ë„¤ëœë€ë“œê³„ í”„ë¡œê·¸ë˜ë¨¸ì¸ ê·€ë„ ë°˜ ë¡œì„¬(Guido van Rossum)ì— ì˜í•´ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŠ” ABC í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ ê°œë°œìì˜€ìœ¼ë©°, íŒŒì´ì¬ì„ ê°œë°œí•  ë•Œ ABCì˜ ì¥ì ì„ ê³„ìŠ¹í•˜ë©´ì„œë„ ë” ì‰½ê³  ì§ê´€ì ì¸ ì–¸ì–´ë¥¼ ë§Œë“¤ê³ ì í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. ì´ëŠ” í”„ë¡œê·¸ë˜ë¨¸ê°€ ë°ì´í„°ë¥¼ ê°ì²´ë¡œ í‘œí˜„í•˜ê³ , ì´ ê°ì²´ë“¤ì´ ìƒí˜¸ì‘ìš©í•˜ë„ë¡ í”„ë¡œê·¸ë˜ë°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. íŒŒì´ì¬ì€ ë˜í•œ ë™ì  íƒ€ì´í•‘ ì–¸ì–´ì…ë‹ˆë‹¤. ì´ëŠ” ë³€ìˆ˜ì˜ íƒ€ì…ì„ ë¯¸ë¦¬ ì„ ì–¸í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
      "\n",
      "### íŠ¹ì§•\n",
      "\n",
      "íŒŒì´ì¬ì˜ ì£¼ìš” íŠ¹ì§• ëª‡ ê°€ì§€ë¥¼ ì†Œê°œí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ì‰¬ìš´ í•™ìŠµ ê³¡ì„ **: íŒŒì´ì¬ì€ ë§¤ìš° ì§ê´€ì ì´ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ˆë³´ìê°€ í”„ë¡œê·¸ë˜ë°ì„ ì‹œì‘í•  ë•Œ ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ëŒ€ê·œëª¨ ë¼ì´ë¸ŒëŸ¬ë¦¬**: íŒŒì´ì¬ì€ ë°©ëŒ€í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ë¶„ì„, ì›¹ ê°œë°œ, ì¸ê³µì§€ëŠ¥, ê³¼í•™ ì»´í“¨íŒ… ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **í”Œë«í¼ ë…ë¦½ì„±**: íŒŒì´ì¬ì€ í”Œë«í¼ì— ë…ë¦½ì ì…ë‹ˆë‹¤. ì¦‰, íŒŒì´ì¬ìœ¼ë¡œ ì‘ì„±ëœ í”„ë¡œê·¸ë¨ì€ Windows, macOS, Linux ë“± ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ì‹¤í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ì˜¤í”ˆ ì†ŒìŠ¤**: íŒŒì´ì¬ì€ ì˜¤í”ˆ ì†ŒìŠ¤ ì–¸ì–´ì…ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œê°€ ê³µê°œë˜ì–´ ìˆì–´ ëˆ„êµ¬ë‚˜ ììœ ë¡­ê²Œ ì‚¬ìš©, ìˆ˜ì •, ë°°í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì»¤ë®¤ë‹ˆí‹° ì§€ì›**: íŒŒì´ì¬ì€ ë§¤ìš° í™œë™ì ì¸ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°œë°œìë“¤ì´ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€, ì½”ë“œ ì˜ˆì œ, íŠœí† ë¦¬ì–¼ ë“± ë‹¤ì–‘í•œ ìì›ì„ ì˜¨ë¼ì¸ì—ì„œ ì‰½ê²Œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì‘ìš© ë¶„ì•¼\n",
      "\n",
      "íŒŒì´ì¬ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "- **ì›¹ ê°œë°œ**: íŒŒì´ì¬ì€ ì›¹ ê°œë°œì„ ìœ„í•´ Django, Flaskì™€ ê°™ì€ ì¸ê¸° ìˆëŠ” í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "- **ë°ì´í„° ê³¼í•™ ë° ì¸ê³µì§€ëŠ¥**: NumPy, pandas, scikit-learn, TensorFlow, PyTorchì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "- **ê³¼í•™ ì»´í“¨íŒ…**: ê³¼í•™ì ì¸ ì»´í“¨íŒ…ì„ ìœ„í•´ scipy, matplotlibì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì œê³µë©ë‹ˆë‹¤.\n",
      "\n",
      "- **êµìœ¡**: íŒŒì´ì¬ì˜ ê°„ë‹¨í•œ ë¬¸ë²•ê³¼ ê°•ë ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì›ìœ¼ë¡œ ì¸í•´, êµìœ¡ í˜„ì¥ì—ì„œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "- **ìë™í™”**: íŒŒì´ì¬ì€ ì‹œìŠ¤í…œ ìœ í‹¸ë¦¬í‹°, ë°ì´í„° ì²˜ë¦¬, ìë™í™”ëœ í…ŒìŠ¤íŠ¸ì™€ ê°™ì€ ì‘ì—… ìë™í™”ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "íŒŒì´ì¬ì€ ê·¸ ìœ ì—°ì„±ê³¼ ê°•ë ¥í•œ ê¸°ëŠ¥ìœ¼ë¡œ ì¸í•´ í”„ë¡œê·¸ë˜ë° ì„¸ê³„ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ìœ„ì¹˜ë¥¼ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ˆë³´ìì™€ ìˆ™ë ¨ëœ ê°œë°œì ëª¨ë‘ì—ê²Œ ì‚¬ë‘ë°›ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(\"ì‘ë‹µ:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750b360",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLMì„ Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70824e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\\n    ')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}ì— ëŒ€í•´ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15b4b6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain ì—°ê²° (LCEL)\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e475b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. \n",
      "\n",
      "ê¸°ë³¸ì ìœ¼ë¡œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ì™€ ê°•ì•„ì§€ì˜ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ë©´ì„œ 'ì´ê±´ ê³ ì–‘ì´ì•¼', 'ì´ê±´ ê°•ì•„ì§€ì•¼'ë¼ê³  ì•Œë ¤ì£¼ëŠ” ê²ë‹ˆë‹¤. \n",
      "\n",
      "ëª¨ë¸ì€ ì´ ë°ì´í„°ë¥¼ ë³´ê³  ìŠ¤ìŠ¤ë¡œ íŠ¹ì§•ì„ ì°¾ìŠµë‹ˆë‹¤. 'ê³ ì–‘ì´ëŠ” ê·€ê°€ ë¾°ì¡±í•˜ê³  ëˆˆì´ í¬ë‹¤', 'ê°•ì•„ì§€ëŠ” ê·€ê°€ ì«‘ê¸‹ ì„œê³  ê¼¬ë¦¬ê°€ ê¸¸ë‹¤'ì™€ ê°™ì€ íŠ¹ì§•ì„ ìŠ¤ìŠ¤ë¡œ ë°œê²¬í•˜ëŠ” ê±°ì£ .\n",
      "\n",
      "ì´ë ‡ê²Œ ì°¾ì€ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì€ 'ê³ ì–‘ì´ì¸ì§€ ê°•ì•„ì§€ì¸ì§€'ë¥¼ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€ì„ ë§Œë“­ë‹ˆë‹¤. \n",
      "\n",
      "ê·¸ëŸ°ë° ì²˜ìŒì—ëŠ” ì´ ê¸°ì¤€ì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ëª¨ë¸ì€ ê³„ì†í•´ì„œ ë°ì´í„°ë¥¼ ë³´ê³  ë˜ ë³´ê³  í•˜ë©´ì„œ ìì‹ ì˜ ê¸°ì¤€ì„ ì¡°ê¸ˆì”© ê°œì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ì²˜ìŒì—ëŠ” ê³ ì–‘ì´ ì‚¬ì§„ì„ ë³´ê³  'ê°•ì•„ì§€ì•¼'ë¼ê³  ì˜ëª» ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ì€ 'ì•„, ë‚´ê°€ ì˜ëª»í–ˆêµ¬ë‚˜'í•˜ê³  ê¹¨ë‹«ê³ , ë‹¤ìŒì—ëŠ” ì¢€ ë” ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©´ì„œ ëª¨ë¸ì€ ì ì  ë” ì •í™•í•´ì§€ê³ , ê²°êµ­ì—ëŠ” ìƒˆë¡œìš´ ì‚¬ì§„ì„ ë³´ê³ ë„ 'ì´ê±´ ê³ ì–‘ì´ì•¼', 'ì´ê±´ ê°•ì•„ì§€ì•¼'ë¼ê³  ì •í™•í•˜ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ì²˜ëŸ¼ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³ , ìŠ¤ìŠ¤ë¡œ íŠ¹ì§•ì„ ì°¾ê³ , ê¸°ì¤€ì„ ë§Œë“¤ì–´ê°€ëŠ” ê³¼ì •ì„ í†µí•´ ë°œì „í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb99721",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM + OutputParserì„ Chainìœ¼ë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3c2d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain ì—°ê²° (LCEL)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eea99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChainì€ ë‹¤ì–‘í•œ AI ê¸°ë°˜ ì œí’ˆ ë° ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” íšŒì‚¬ì…ë‹ˆë‹¤. LangChainì˜ ì£¼ìš” ì œí’ˆì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
      "\n",
      "1. **LangChain**: ë­ì²´ì¸ í”Œë«í¼ì€ ê°œë°œìê°€ ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì„ ì‰½ê²Œ êµ¬ì¶•, í†µí•© ë° ë°°í¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ í”Œë«í¼ì€ ì‚¬ì „ êµ¬ì¶•ëœ êµ¬ì„± ìš”ì†Œì™€ íˆ´ì„ ì œê³µí•˜ì—¬ ê°œë°œìê°€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **LangServe**: LangServeëŠ” LangChainì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” API ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì´ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ê°œë°œìëŠ” ë³„ë„ì˜ ì¸í”„ë¼ë¥¼ êµ¬ì¶•í•˜ì§€ ì•Šê³ ë„ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê²Œ ë°°í¬í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **LangSmith**: ë­ìŠ¤ë¯¸ìŠ¤ëŠ” ê°œë°œìê°€ LangChainì„ ì‚¬ìš©í•˜ì—¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë” ë¹ ë¥´ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” íˆ´ì…ë‹ˆë‹¤. LangSmithë¥¼ ì‚¬ìš©í•˜ë©´ ì›Œí¬í”Œë¡œìš° ê°œë°œ, í…ŒìŠ¤íŠ¸ ë° ë°°í¬ë¥¼ ê°„ì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì´ëŸ¬í•œ ì œí’ˆë“¤ì€ LangChainì´ ì œê³µí•˜ëŠ” ì£¼ìš” ì„œë¹„ìŠ¤ì´ë©°, ê°œë°œìê°€ ìì—°ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ì„ ê°–ì¶˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë” ì‰½ê²Œ êµ¬ì¶•í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# chain í˜¸ì¶œ\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChainì˜ Products(ì œí’ˆ)ëŠ” ì–´ë–¤ ê²ƒë“¤ì´ ìˆë‚˜ìš”?\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2945fc",
   "metadata": {},
   "source": [
    "### Runnableì˜ stream() í•¨ìˆ˜ í˜¸ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d29354a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ì‚¬ëŒì˜ ë‡Œê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì‚¬ëŒì€ ê²½í—˜ì„ í†µí•´ í•™ìŠµí•˜ê³ , ì»´í“¨í„°ë„ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "\n",
      "**ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •**\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤. ì´ ë°ì´í„°ëŠ” ë¬¸ì œì— ëŒ€í•œ ë‹µì„ ì•Œê³  ìˆëŠ” ë°ì´í„°ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**: ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³ , í•„ìš”í•œ ê²½ìš° ë°ì´í„°ë¥¼ ë³€í™˜í•˜ì—¬ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì‰½ê²Œ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
      "3. **ëª¨ë¸ ì´ˆê¸°í™”**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì˜ êµ¬ì¡°ì™€ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
      "4. **í•™ìŠµ**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì´ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
      "5. **í‰ê°€**: í•™ìŠµëœ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³ , ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
      "\n",
      "**ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ë°©ë²•**\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ì§€ë„ í•™ìŠµ**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì œê³µí•˜ê³ , ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ ì•Œë ¤ì£¼ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì€ ë°ì´í„°ì™€ ë‹µ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.\n",
      "2. **ë¹„ì§€ë„ í•™ìŠµ**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì œê³µí•˜ì§€ë§Œ, ë°ì´í„°ì— ëŒ€í•œ ë‹µì„ ì•Œë ¤ì£¼ì§€ ì•ŠëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì€ ë°ì´í„°ì˜ íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ ë°œê²¬í•©ë‹ˆë‹¤.\n",
      "3. **ê°•í™” í•™ìŠµ**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì´ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©°, ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
      "\n",
      "**ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬**\n",
      "\n",
      "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ì˜¤ì°¨ ìµœì†Œí™”**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ë ¤ê³  í•©ë‹ˆë‹¤.\n",
      "2. **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ ê°€ì¤‘ì¹˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "3. **íŒ¨í„´ ë°œê²¬**: ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ë°œê²¬í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ í†µí•´ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ë¶„ë¥˜ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤ê³  ê°€ì •í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ê³ ì–‘ì´, ê°œ, ìë™ì°¨ ë“±ì˜ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
      "\n",
      "* **ë°ì´í„° ìˆ˜ì§‘**: ë‹¤ì–‘í•œ ê³ ì–‘ì´, ê°œ, ìë™ì°¨ ë“±ì˜ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
      "* **ë°ì´í„° ì „ì²˜ë¦¬**: ì´ë¯¸ì§€ë¥¼ ì •ì œí•˜ê³ , í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
      "* **ëª¨ë¸ ì´ˆê¸°í™”**: ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
      "* **í•™ìŠµ**: ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ í•™ìŠµí•˜ê³ , íŒ¨í„´ì„ ë°œê²¬í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
      "* **í‰ê°€**: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì´ë ‡ê²Œ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê³ , íŒ¨í„´ì„ ë°œê²¬í•˜ê³ , ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒí•©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "# ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ì„ ìœ„í•œ ìš”ì²­\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”\"})\n",
    "\n",
    "    for token in answer:\n",
    "        # ìŠ¤íŠ¸ë¦¼ì—ì„œ ë°›ì€ ë°ì´í„°ì˜ ë‚´ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ì„œ ì¶œë ¥í•˜ê³ , ë²„í¼ë¥¼ ì¦‰ì‹œ ë¹„ì›ë‹ˆë‹¤.\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c0b5bd",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* ì²«ë²ˆì§¸ Chainì˜ ì¶œë ¥ì´, ë‘ë²ˆì§¸ Chainì˜ ì…ë ¥ì´ ëœë‹¤.\n",
    "* ë‘ê°œì˜ Chainê³¼ Prompt + OutputParserë¥¼ LCELë¡œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45fc01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¥ë¥´ì— ë”°ë¼ ì˜í™” ì¶”ì²œ\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} ì¥ë¥´ì—ì„œ ì¶”ì²œí•  ë§Œí•œ ì˜í™”ë¥¼ í•œ í¸ ì•Œë ¤ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# Step 2: ì¶”ì²œëœ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ ìš”ì•½\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} ì¶”ì „í•œ ì˜í™”ì˜ ì œëª©ì„ ë¨¼ì € ì•Œë ¤ì£¼ì‹œê³ , ì¤„ì„ ë°”ê¾¸ì–´ì„œ ì˜í™”ì˜ ì¤„ê±°ë¦¬ë¥¼ 10ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ì²´ì¸ 1: ì˜í™” ì¶”ì²œ (ì…ë ¥: ì¥ë¥´ â†’ ì¶œë ¥: ì˜í™” ì œëª©)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c2ac66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜í™”ì˜ ì œëª©ì€ 'ì¡´ ìœ…'ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì „ì§ ì•”ì‚´ì ì¡´ ìœ…ì€ ì€í‡´ í›„ í‰í™”ë¡œìš´ ì‚¶ì„ ì‚´ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŠ” ì•„ë‚´ë¥¼ ë¨¼ì € ìƒê³ , ì–¼ë§ˆ ì§€ë‚˜ì§€ ì•Šì•„ ì•”ìœ¼ë¡œ ì•„ë‚´ê°€ ì‚¬ë§í•©ë‹ˆë‹¤.\n",
      "\n",
      "ì¡´ì€ ì•„ë‚´ê°€ ë‚¨ê¸´ ë§ˆì§€ë§‰ ì„ ë¬¼ì¸ ê°•ì•„ì§€ ë°ì´ì§€ì™€ í•¨ê»˜ ì™¸ë¡œì›€ì„ ë‹¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŸ°ë°, ì¡´ì˜ ì§‘ì— ë¬´ë‹¨ ì¹¨ì…í•œ ì´ì˜¤ì‹ (ì˜ê³ ë“œ)ì€ ì¡´ì˜ ì°¨ë¥¼ í›”ì¹˜ê³ , ë°ì´ì§€ê¹Œì§€ ì£½ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì´ì˜¤ì‹ ì€ ê³ ìœ„ ì¡°ì§ì›ì¸ ì‚°í‹°ì•„ê³ ì˜ ì•„ë“¤ì´ë©°, ì¡´ì˜ ì°¨ëŠ” ì‚°í‹°ì•„ê³ ê°€ ì„ ë¬¼ë¡œ ì¤€ ì°¨ëŸ‰ì…ë‹ˆë‹¤.\n",
      "\n",
      "ì¡´ì€ ë°ì´ì§€ì˜ ì£½ìŒì— ë³µìˆ˜ë¥¼ ë‹¤ì§í•˜ë©°, ì „ì§ ì•”ì‚´ìì˜ ì‚¶ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "\n",
      "ê·¸ëŠ” ì‚°í‹°ì•„ê³ ì˜ ì¡°ì§ê³¼ ì „ìŸì„ ë²Œì´ë©°, ë§ì€ ì ë“¤ì„ ìƒëŒ€ë¡œ ì‹¸ì›ë‹ˆë‹¤.\n",
      "\n",
      "ì¡´ì€ ì „ ì„¸ê³„ì˜ ì•”ì‚´ ì¡°ì§ë“¤ë¡œë¶€í„° ì«“ê¸°ê²Œ ë˜ë©°, ê·¸ì˜ ëª©ì—ëŠ” ì—„ì²­ë‚œ í˜„ìƒê¸ˆì´ ê±¸ë¦½ë‹ˆë‹¤.\n",
      "\n",
      "ì¡´ì€ ë™ë£Œë“¤ì˜ ë„ì›€ì„ ë°›ì•„ê°€ë©°, ì‚°í‹°ì•„ê³ ì™€ ê·¸ì˜ ì¡°ì§ì„ ìƒëŒ€ë¡œ ì¹˜ì—´í•œ ì „íˆ¬ë¥¼ ë²Œì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ì²´ì¸ 2: ì¤„ê±°ë¦¬ ìš”ì•½ (ì…ë ¥: ì˜í™” ì œëª© â†’ ì¶œë ¥: ì¤„ê±°ë¦¬)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1ì˜ ì¶œë ¥ì„ movie ì…ë ¥ ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # ì‹¤í–‰: \"SF\" ì¥ë¥´ì˜ ì˜í™” ì¶”ì²œ ë° ì¤„ê±°ë¦¬ ìš”ì•½\n",
    "    response = chain2.invoke({\"genre\": \"Action\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7ac10",
   "metadata": {},
   "source": [
    "### PromptTemplate ì—¬ëŸ¬ ê°œ ì—°ê²°í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "897f35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ChatGPTëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œì„œ, ì£¼ì–´ì§„ ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ìŒì— ì˜¬ ìˆ˜ ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ GPT ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë©°, ì…€í”„ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ ì…ë ¥ ë¬¸ì¥ì˜ ê° í† í° ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—…ì— ëŒ€í•œ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "ChatGPT ëª¨ë¸ì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "* ìì—°ìŠ¤ëŸ½ê³  ìœ ì°½í•œ í…ìŠ¤íŠ¸ ìƒì„±\n",
      "* ë‹¤ì–‘í•œ ì£¼ì œì™€ ìŠ¤íƒ€ì¼ì— ëŒ€í•œ ì´í•´\n",
      "* ìƒí™©ì— ë§ëŠ” ëŒ€í™” ê°€ëŠ¥\n",
      "* ë†’ì€ ìˆ˜ì¤€ì˜ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥\n",
      "\n",
      "ChatGPT ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ë‹¤ìŒê³¼ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "* BERT\n",
      "* RoBERTa\n",
      "* LLaMA\n",
      "* PaLM\n",
      "\n",
      "ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ ëŒ€ë¶€ë¶„ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì„œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"ì˜ì–´\")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"ì˜ì–´\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70d3d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 2 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'Gemma ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 3 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.', 'llama-4 ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ 4 ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "547b9a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='GPT-4 ëª¨ë¸ì€ ëŒ€ê·œëª¨ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì–¸ì–´ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ìµíˆëŠ” ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹¤ìŒì— ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ìì—°ìŠ¤ëŸ¬ìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 28, 'total_tokens': 89, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.37781006500000003, 'prompt_time': 0.002923378, 'completion_time': 0.147120252, 'total_time': 0.15004363}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-282ead74-8017-475c-9970-86f2959d8892', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--6e3391b2-a4e4-4f57-a719-3de31bee1d2b-0' usage_metadata={'input_tokens': 28, 'output_tokens': 61, 'total_tokens': 89, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='GemmaëŠ” ì»´í“¨í„°ê°€ ìì—°ì–´ ì‘ì—…ì„ ë” ì˜ ìˆ˜í–‰í•˜ë„ë¡ ë•ëŠ” ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ì˜ í†µê³„ì™€ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ëŠ” ë§ì€ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ êµìœ¡ì„ í†µí•´ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 27, 'total_tokens': 86, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.34626035800000005, 'prompt_time': 0.002931598, 'completion_time': 0.140381372, 'total_time': 0.14331297}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-22cb8d14-2ddc-4fc3-929c-ef762138690f', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--a53e3546-43fe-44fe-a57e-633b9e316963-0' usage_metadata={'input_tokens': 27, 'output_tokens': 59, 'total_tokens': 86, 'input_token_details': {}, 'output_token_details': {}}\n",
      "content='llama-4 ëª¨ë¸ì€ ë©”íƒ€ì—ì„œ ê°œë°œí•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë°©ëŒ€í•œ ì–‘ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë˜ë©°, ì´ë¥¼ í†µí•´ ìì—°ì–´ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ìŠµë“í•©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë¬¸ë§¥ì—ì„œ ë‹¤ìŒì— ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë©°, ì´ ê³¼ì •ì„ í†µí•´ ì–¸ì–´ì˜ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ llama-4 ëª¨ë¸ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 29, 'total_tokens': 119, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.27866622799999996, 'prompt_time': 0.003018367, 'completion_time': 0.215626779, 'total_time': 0.218645146}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-a3edc413-48f0-480a-a7db-17e0661f5451', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--22145599-593e-460f-8b35-7e16ad681d34-0' usage_metadata={'input_tokens': 29, 'output_tokens': 90, 'total_tokens': 119, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f796de",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "* SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f44f16cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I wrote it to provide a comprehensive overview of deep learning.\n",
      "\n",
      "**Deep Learning: A Comprehensive Overview**\n",
      "=====================================\n",
      "\n",
      "### Introduction\n",
      "\n",
      "Deep learning is a subset of machine learning, which is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed. It is a key technology behind many recent advances in AI, including image and speech recognition, natural language processing, and autonomous vehicles.\n",
      "\n",
      "### What is Deep Learning?\n",
      "\n",
      "Deep learning is a type of machine learning that uses neural networks to analyze data. A neural network is a computer system inspired by the structure and function of the human brain. It consists of layers of interconnected nodes or \"neurons,\" which process and transmit information.\n",
      "\n",
      "In traditional machine learning, a computer is trained on a dataset and then uses that training to make predictions on new, unseen data. In deep learning, the computer is trained on a large dataset and learns to represent the data in a hierarchical manner, with early layers learning low-level features and later layers learning higher-level features.\n",
      "\n",
      "### Key Characteristics of Deep Learning\n",
      "\n",
      "* **Multiple Layers**: Deep learning models typically have multiple layers, which allow them to learn complex patterns in data.\n",
      "* **Neural Networks**: Deep learning models are based on neural networks, which are composed of nodes or \"neurons\" that process and transmit information.\n",
      "* **Large Datasets**: Deep learning models require large amounts of data to train effectively.\n",
      "* **Computational Power**: Deep learning models require significant computational power to train and deploy.\n",
      "\n",
      "### Types of Deep Learning Models\n",
      "\n",
      "* **Convolutional Neural Networks (CNNs)**: CNNs are commonly used for image and video analysis tasks, such as image classification, object detection, and image segmentation.\n",
      "* **Recurrent Neural Networks (RNNs)**: RNNs are commonly used for sequential data, such as speech, text, and time series data.\n",
      "* **Autoencoders**: Autoencoders are used for unsupervised learning tasks, such as dimensionality reduction and anomaly detection.\n",
      "\n",
      "### Applications of Deep Learning\n",
      "\n",
      "* **Computer Vision**: Deep learning is widely used in computer vision applications, such as image recognition, object detection, and image segmentation.\n",
      "* **Natural Language Processing**: Deep learning is used in natural language processing applications, such as language translation, sentiment analysis, and text summarization.\n",
      "* **Speech Recognition**: Deep learning is used in speech recognition applications, such as voice assistants and speech-to-text systems.\n",
      "\n",
      "### Benefits of Deep Learning\n",
      "\n",
      "* **Improved Accuracy**: Deep learning models can achieve state-of-the-art accuracy on many tasks.\n",
      "* **Flexibility**: Deep learning models can be applied to a wide range of tasks and domains.\n",
      "* **Scalability**: Deep learning models can be scaled up to handle large datasets and complex tasks.\n",
      "\n",
      "### Challenges of Deep Learning\n",
      "\n",
      "* **Data Requirements**: Deep learning models require large amounts of data to train effectively.\n",
      "* **Computational Power**: Deep learning models require significant computational power to train and deploy.\n",
      "* **Interpretability**: Deep learning models can be difficult to interpret and understand.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Deep learning is a powerful technology that has enabled many recent advances in AI. Its ability to learn complex patterns in data has made it a key tool in many applications, from computer vision and natural language processing to speech recognition and autonomous vehicles. However, deep learning also presents challenges, such as data requirements and computational power, that must be addressed in order to fully realize its potential.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an {topic} expert. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a6ded5",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* ì˜ˆì‹œë¥¼ ì œê³µ í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9c8db67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### íƒœì–‘ê³„ í–‰ì„±\n",
      "1. **ìˆ˜ì„±**: íƒœì–‘ê³¼ ê°€ì¥ ê°€ê¹Œìš´ í–‰ì„±ìœ¼ë¡œ, ë§¤ìš° ì‘ê³  ì˜¨ë„ê°€ ê·¹ì‹¬í•˜ê²Œ ë³€í•©ë‹ˆë‹¤.\n",
      "2. **ê¸ˆì„±**: ë°ê³  ëœ¨ê±°ìš´ í–‰ì„±ìœ¼ë¡œ, ê°•í•œ ì˜¨ì‹¤ íš¨ê³¼ë¡œ í‘œë©´ ì˜¨ë„ê°€ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.\n",
      "3. **ì§€êµ¬**: ìƒëª…ì²´ê°€ ì¡´ì¬í•˜ëŠ” ìœ ì¼í•œ í–‰ì„±ìœ¼ë¡œ, ëŒ€ê¸° êµ¬ì„±ê³¼ ë¬¼ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "4. **í™”ì„±**: ë¶‰ì€ í–‰ì„±ìœ¼ë¡œ, ê³¼ê±°ì—ëŠ” ë¬¼ì´ ìˆì—ˆë‹¤ê³  ì¶”ì •ë˜ë©° í˜„ì¬ íƒì‚¬ê°€ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.\n",
      "5. **ëª©ì„±**: íƒœì–‘ê³„ì—ì„œ ê°€ì¥ í° í–‰ì„±ìœ¼ë¡œ, ê°€ìŠ¤ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "6. **í† ì„±**: ì•„ë¦„ë‹¤ìš´ ê³ ë¦¬ë¥¼ ê°€ì§„ ê°€ìŠ¤ í–‰ì„±ì…ë‹ˆë‹¤.\n",
      "7. **ì²œì™•ì„±**: ë¹™í•˜ í–‰ì„±ìœ¼ë¡œ, ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
      "8. **í•´ì™•ì„±**: ê°€ì¥ ë¨¼ í–‰ì„±ìœ¼ë¡œ, ê°•í•œ ë°”ëŒê³¼ ì¶”ìš´ ì˜¨ë„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\"})\n",
    "#result = chain.invoke({\"input\": \"ì–‘ì ì–½í˜ì´ ë¬´ì—‡ì¸ê°€ìš”?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4c453",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* í”„ë¡¬í”„íŠ¸ì˜ ì…ë ¥ ê°’ì— í•¨ìˆ˜ í˜¸ì¶œì´ë‚˜ ì™¸ë¶€ APIë¥¼ í˜¸ì¶œí•œ ë™ì ì¸ ê°’ì„ ëŒ€ì…í•  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e4bf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í”„ë¡¬í”„íŠ¸: ê²¨ìš¸ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ íƒœí’ ë°œìƒì…ë‹ˆë‹¤.\n",
      "ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: ê²¨ìš¸ì— íƒœí’ì´ ë°œìƒí•˜ëŠ” ê²ƒì€ ë“œë¬¸ ì¼ì…ë‹ˆë‹¤. íƒœí’ì€ ì¼ë°˜ì ìœ¼ë¡œ ì—¬ë¦„ê³¼ ì´ˆê°€ì„ì— ë°œìƒí•©ë‹ˆë‹¤. íƒœí’ì€ ì—´ëŒ€ì§€ë°©ì˜ ë”°ëœ»í•œ ë°”ë‹¤ì—ì„œ ë°œìƒí•˜ê¸° ë•Œë¬¸ì—, ê²¨ìš¸ì—ëŠ” íƒœí’ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
      "\n",
      "ê²¨ìš¸ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   ë¶ê·¹ ì†Œìš©ëŒì´\n",
      "*   ì˜¤ë¡œë¼\n",
      "*   ëˆˆê³¼ ì–¼ìŒì˜ í˜•ì„±\n",
      "*   ë‚®ì´ ì§§ì•„ì§ ë“±ì— ë”°ë¥¸ ê¸°ìƒ ë³€í™”\n",
      "\n",
      "ì´ëŸ¬í•œ í˜„ìƒë“¤ì€ ì§€êµ¬ì˜ ìì „ê³¼ ê³µì „, íƒœì–‘ì˜ ë³µì‚¬ ì—ë„ˆì§€ ë“±ì— ì˜í•´ ë°œìƒí•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì…ë‹ˆë‹¤.\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season(\"south\")}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")  # 'íƒœí’ ë°œìƒ'ì€ ì—¬ë¦„ê³¼ ê´€ë ¨ë¨\n",
    "result = model.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\"ğŸ”¹ í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e13bf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ í”„ë¡¬í”„íŠ¸: í˜„ì¬ 1ë‹¬ëŸ¬ = 1365.14ì› ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œ ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n",
      "ğŸ”¹ ëª¨ë¸ ì‘ë‹µ: ## 1. í˜„ì¬ í™˜ìœ¨ ì •ë³´\n",
      "\n",
      "*   í˜„ì¬ 1ë‹¬ëŸ¬ = 1365.14ì› (ê¸°ì¤€: 2024ë…„ 4ì›” 5ì¼ 12ì‹œ ê¸°ì¤€, í•œêµ­ì€í–‰)\n",
      "\n",
      "## 2. í•œêµ­ ê²½ì œì— ë¯¸ì¹˜ëŠ” ì˜í–¥\n",
      "\n",
      "*   **ìˆ˜ì¶œ ì¦ê°€**: ì•½ë‹¬ëŸ¬ëŠ” í•œêµ­ì˜ ìˆ˜ì¶œì„ ì¦ê°€ ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ë¬¼ê°€ ìƒìŠ¹**: ì›í™” ì•½ì„¸ë¡œ ì¸í•´ ìˆ˜ì… ë¬¼ê°€ê°€ ìƒìŠ¹í•˜ì—¬ êµ­ë‚´ ë¬¼ê°€ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ì™¸êµ­ì¸ íˆ¬ì ê°ì†Œ**: ì›í™” ì•½ì„¸ë¡œ ì¸í•´ ì™¸êµ­ì¸ íˆ¬ìê°€ ê°ì†Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ê²½ì œ ì„±ì¥ë¥ ì— ì˜í–¥**: í™˜ìœ¨ ë³€ë™ì€ ê²½ì œ ì„±ì¥ë¥ ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "## 3. í–¥í›„ í™˜ìœ¨ ì˜ˆìƒê°’ ë¶„ì„\n",
      "\n",
      "*   **ê²½ì œ ìƒí™©**: ë¯¸êµ­ì˜ ê²½ì œ ìƒí™©ê³¼ í•œêµ­ì˜ ê²½ì œ ìƒí™© ë“±ì„ ê³ ë ¤í•˜ì—¬ í™˜ìœ¨ì„ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ê¸ˆë¦¬ ë³€ë™**: ë¯¸êµ­ê³¼ í•œêµ­ì˜ ê¸ˆë¦¬ ë³€ë™ì— ë”°ë¼ í™˜ìœ¨ì´ ë³€ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©**: ê¸€ë¡œë²Œ ê²½ì œ ìƒí™©ê³¼ êµ­ì œ ë¬´ì—­ ë“±ì— ë”°ë¼ í™˜ìœ¨ì´ ë³€ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "*   **ì˜ˆìƒê°’**: í–¥í›„ í™˜ìœ¨ ì˜ˆìƒê°’ì€ 1300ì› ëŒ€ì—ì„œ 1400ì› ëŒ€ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ì˜ˆì¸¡ì´ë©° ì‹¤ì œ í™˜ìœ¨ì€ ë‹¤ì–‘í•œ ìš”ì¸ì— ì˜í•´ ë³€ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "## 4. ê²°ë¡ \n",
      "\n",
      "*   í˜„ì¬ í™˜ìœ¨ì€ í•œêµ­ ê²½ì œì— ë‹¤ì–‘í•œ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. í–¥í›„ í™˜ìœ¨ ì˜ˆìƒê°’ì€ ë‹¤ì–‘í•œ ìš”ì¸ì— ì˜í•´ ë³€ë™í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# {info} ë³€ìˆ˜ì— APIì—ì„œ ë°›ì€ í™˜ìœ¨ ì •ë³´ë¥¼ ë™ì ìœ¼ë¡œ ë°˜ì˜\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. í˜„ì¬ í™˜ìœ¨ì„ ê¸°ì¤€ìœ¼ë¡œ í•œêµ­ê²½ì œ ë¯¸ì¹˜ëŠ” ì˜í–¥ ë° í–¥í›„ì— í™˜ìœ¨ì˜ ì˜ˆìƒê°’ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ”¹ í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\"ğŸ”¹ ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
