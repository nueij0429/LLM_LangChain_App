{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7245f102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ae1c",
   "metadata": {},
   "source": [
    "### deepseek-r1:1.5b 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aff0926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " 'Alright, the user asked about \"LangChain.\" I know it\\'s a Python library for '\n",
      " 'LLMs, so I should explain what that means. Let me break it down.\\n'\n",
      " '\\n'\n",
      " \"First, I'll mention LangChain is built on open-source libraries like PyTorch \"\n",
      " \"and TensorFlow. It's designed to simplify working with language models \"\n",
      " 'because of its flexible architecture. That makes sense.\\n'\n",
      " '\\n'\n",
      " 'I should list the main features to give a clear picture. Maybe include '\n",
      " 'things like easy model management, integration into pipelines, customizable '\n",
      " 'training functions, and support for multiple languages or dialects. These '\n",
      " 'points will help users understand what they can do with LangChain.\\n'\n",
      " '\\n'\n",
      " 'I need to make sure I cover installation, configuration, data handling, '\n",
      " \"deployment options, and community support since that's important. Also, \"\n",
      " 'mentioning the open-source nature gives credit to the developers for '\n",
      " 'high-quality code.\\n'\n",
      " '\\n'\n",
      " \"Finally, I'll offer more information if needed so the user feels comfortable \"\n",
      " 'asking further questions.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a Python library designed to simplify working with large '\n",
      " 'language models (LLMs). It provides a flexible and intuitive architecture '\n",
      " 'for building and managing LLM-based applications. Here are some key features '\n",
      " 'of LangChain:\\n'\n",
      " '\\n'\n",
      " '1. **Open-Source Underlying Libraries**: LangChain leverages open-source '\n",
      " 'libraries like PyTorch, TensorFlow, and NLTK under the hood. This ensures '\n",
      " 'that it is well-supported by a large community of developers.\\n'\n",
      " '\\n'\n",
      " '2. **Simplified Model Management**: LangChain allows users to easily manage '\n",
      " 'their LLM models through its modular architecture. Users can create custom '\n",
      " 'model configurations or use pre-trained models from popular frameworks like '\n",
      " \"HuggingFace's transformers.\\n\"\n",
      " '\\n'\n",
      " '3. **Integration with Machine Learning Pipelines**: LangChain is designed to '\n",
      " 'work seamlessly with machine learning pipelines, enabling the building of '\n",
      " 'applications that integrate text generation, classification, and other '\n",
      " 'tasks.\\n'\n",
      " '\\n'\n",
      " '4. **Customizable Training Functions**: Users can define their own training '\n",
      " 'functions for LLMs, allowing them to customize the training process based on '\n",
      " 'their specific needs.\\n'\n",
      " '\\n'\n",
      " '5. **Support for Multiple Languages/Dialects**: LangChain supports working '\n",
      " 'with language models in multiple languages or dialects through a simple '\n",
      " 'configuration layer.\\n'\n",
      " '\\n'\n",
      " '6. **Flexible Deployment Options**: The library is extensible and can be '\n",
      " 'deployed as both local and serverless applications, making it suitable for '\n",
      " 'use cases ranging from on-premises environments to cloud-based setups.\\n'\n",
      " '\\n'\n",
      " \"If you'd like more details about any of these features or need help with \"\n",
      " 'specific aspects of LangChain, feel free to ask!')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf15845",
   "metadata": {},
   "source": [
    "### qwen2.5 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23329af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('파이썬은 컴퓨터 programming language (프로그래밍 언어)입니다. 이는 코드를 작성하고 실행하는 방법을 제공하는 특정 '\n",
      " '텍스트 파일 또는 프로그램 집합입니다.\\n'\n",
      " '\\n'\n",
      " '이 언어의 특징으로는:\\n'\n",
      " '\\n'\n",
      " '1. 명료한 설명을 요구하지 않음: 파이썬은 대개 명확하게 구조화된 코드를 사용하기 때문에 문장이 짧고 간결합니다.\\n'\n",
      " '\\n'\n",
      " '2. 인터페이스 평평함: 모든 기능들은 단일 함수나 메소드로 제공되며, 이는 애플리케이션의 분산화를 가능하게 합니다.\\n'\n",
      " '\\n'\n",
      " '3. 가볍고 높은 코드 쓰기 효율성: 파이썬은 실행 시간을 줄이는 스크립트 언어 중 하나로 알려져 있습니다.\\n'\n",
      " '\\n'\n",
      " '4. 국제화 및 지역화 지원: 파이썬은 다양한 언어와 문화를 지원하기 때문에 이는 글로벌 프로그래밍 환경에서 큰 이점을 제공합니다.\\n'\n",
      " '\\n'\n",
      " '5. 교육에 사용됨: 많은 대학과 학원에서 파이썬을 학습 대상으로 사용하여 컴퓨터 과학의 이해를 돕습니다.\\n'\n",
      " '\\n'\n",
      " '6. 빌드 및 배포 편리함: 설치가 필요 없이 쉽게 설치되어, 배포나 업데이트도 간단합니다.\\n'\n",
      " '\\n'\n",
      " '7. 개방형 프로젝트: 파이썬은 다양한 패키지와 모듈을 포함하여 이어진 프로젝트를 만들 수 있게 해 주므로, 복잡한 프로그래밍 작업을 '\n",
      " '쉽게 처리할 수 있습니다.\\n'\n",
      " '\\n'\n",
      " '8. 테스트 용량 증가: 테스트 코드를 작성하는 데 사용되는 자동화 프로그램이 훨씬 간편하게 작동하기 때문에 테스트 시간을 줄일 수 있는 '\n",
      " '장점도 있습니다.\\n'\n",
      " '\\n'\n",
      " '따라서 파이썬은 시스템 개발, 웹 배포, 데이터 분석 및 여러 다른 프로젝트를 위한 다양한 용도로 널리 사용되는 고급 프로그래밍 '\n",
      " '언어입니다.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dab6e6",
   "metadata": {},
   "source": [
    "### qwen3:1.7b 모델 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a6b23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking what Python is, but they want the answer in Korean. Let me start by recalling the basic definition of Python. Python is a programming language, right? It's known for being easy to learn and powerful. I should mention that it's widely used in various fields like web development, data analysis, artificial intelligence, and more.\n",
      "\n",
      "Wait, the user might not know the basics, so I should explain it in simple terms. Maybe start with the word \"파이썬\" and then describe its features. Also, include that it's open-source and has a large community. Oh, and mention that it's interpreted, which means you don't need a compiler. That's a key point.\n",
      "\n",
      "I should also note that Python is object-oriented and has a simple syntax, which makes it accessible for beginners. Maybe add that it's used in projects like machine learning and data science. Oh, and the motto \"Readability counts\" is a good point to include.\n",
      "\n",
      "Wait, should I mention specific examples? Like, maybe mention that it's used in Django for web apps or TensorFlow for AI. But the user might not need specific examples. Keep it general.\n",
      "\n",
      "Also, check if there's any recent updates or features. Python 3.10 introduced some new features, but maybe that's too technical. The user might just want the core information.\n",
      "\n",
      "Make sure the answer is clear and concise. Avoid jargon. Use simple language. Alright, structure the answer with a definition, key features, and applications. That should cover it.\n",
      "</think>\n",
      "\n",
      "파이썬은 프로그래밍 언어로, 간결하고 직관적인 문법을 특징으로 합니다. 컴퓨터로 실행되는 코드를 작성하기 위해 사용되며, 웹 개발, 데이터 분석, 인공지능(AI), 라이브러리 개발 등 다양한 분야에서 널리 활용됩니다.  \n",
      "파이썬은 오픈소스로, 커뮤니티가 활발히 참여하여 다양한 라이브러리와 툴이 제공됩니다. 또한, 코드의 읽기 쉬움과 가독성에 강점을 갖추고 있어 신입 개발자나 기초적인 프로그래밍을 시작하는 사람들에게 적합합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요? 한국어로 답변해주세요.\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecc4720",
   "metadata": {},
   "source": [
    "### LangGraph - Deepseek와 Qwen 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "964df783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "Both numbers have the same whole number part, which is 9. To determine which decimal is larger, I should look at the tenths place of each.\n",
      "\n",
      "In 9.9, the digit in the tenths place is 9.\n",
      "In 9.11, the digit in the tenths place is 1.\n",
      "\n",
      "Since 9 is greater than 1, it follows that 9.9 is larger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
      "\n",
      "### Step 1: Understand the Numbers\n",
      "- **9.9** can be written as **9.90** to have the same decimal places as **9.11**.\n",
      "- **9.11** remains **9.11**.\n",
      "\n",
      "### Step 2: Compare the Whole Number Part\n",
      "Both numbers have the same whole number part:\n",
      "- **Whole number:** 9\n",
      "\n",
      "Since the whole number parts are equal, we need to compare the decimal parts.\n",
      "\n",
      "### Step 3: Compare the Tenths Place\n",
      "- In **9.90**, the tenths digit is **9**.\n",
      "- In **9.11**, the tenths digit is **1**.\n",
      "\n",
      "Comparing these:\n",
      "- **9 > 1**\n",
      "\n",
      "Since **9** (from **9.90**) is greater than **1** (from **9.11**), we can conclude that:\n",
      "\n",
      "\\[\n",
      "\\boxed{9.9 \\text{ is bigger}}\n",
      "\\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0caef49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the two numbers: 9.9 and 9.11.\n",
       "\n",
       "Both numbers have the same whole number part, which is 9. To determine which decimal is larger, I should look at the tenths place of each.\n",
       "\n",
       "In 9.9, the digit in the tenths place is 9.\n",
       "In 9.11, the digit in the tenths place is 1.\n",
       "\n",
       "Since 9 is greater than 1, it follows that 9.9 is larger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is bigger between **9.9** and **9.11**, let's compare them step by step.\n",
       "\n",
       "### Step 1: Understand the Numbers\n",
       "- **9.9** can be written as **9.90** to have the same decimal places as **9.11**.\n",
       "- **9.11** remains **9.11**.\n",
       "\n",
       "### Step 2: Compare the Whole Number Part\n",
       "Both numbers have the same whole number part:\n",
       "- **Whole number:** 9\n",
       "\n",
       "Since the whole number parts are equal, we need to compare the decimal parts.\n",
       "\n",
       "### Step 3: Compare the Tenths Place\n",
       "- In **9.90**, the tenths digit is **9**.\n",
       "- In **9.11**, the tenths digit is **1**.\n",
       "\n",
       "Comparing these:\n",
       "- **9 > 1**\n",
       "\n",
       "Since **9** (from **9.90**) is greater than **1** (from **9.11**), we can conclude that:\n",
       "\n",
       "\\[\n",
       "\\boxed{9.9 \\text{ is bigger}}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "046a955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have whole numbers and decimal parts. Let me break them down.\n",
      "\n",
      "First, 9.9. The whole number part is 9, and the decimal part is 0.9. Then 9.11. The whole number part is 9, and the decimal part is 0.11. Wait, so the decimal parts are 0.9 and 0.11. Hmm, 0.9 is larger than 0.11 because 0.9 is closer to 1. So even though both numbers have the same whole number part, the decimal part of 9.9 is bigger. Therefore, 9.9 should be larger than 9.11.\n",
      "\n",
      "But let me double-check. Maybe I should compare them digit by digit. Let's write them out:\n",
      "\n",
      "9.9 is the same as 9.90. So comparing 9.90 and 9.11. The first decimal place is 9 vs 1. Since 9 is greater than 1, the first number is larger. So yes, 9.9 is bigger than 9.11.\n",
      "\n",
      "I think that's right. There's no need to consider the whole numbers because they're the same. So the answer is 9.9.\n",
      "</think>\n",
      "\n",
      "9.9는 9.11보다 더 큰 수입니다. 두 수의 정수 부분은 모두 9입니다. 소수 부분을 비교하면, 9.9는 0.9, 9.11은 0.11입니다. 0.9는 0.11보다 크므로, 9.9가 더 큰 수입니다.\n",
      "\n",
      "**답변:** 9.9가 더 큰 수입니다."
     ]
    }
   ],
   "source": [
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9와 9.11 중 무엇이 더 큰가요?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52f72f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have whole numbers and decimal parts. Let me break them down.\n",
       "\n",
       "First, 9.9. The whole number part is 9, and the decimal part is 0.9. Then 9.11. The whole number part is 9, and the decimal part is 0.11. Wait, so the decimal parts are 0.9 and 0.11. Hmm, 0.9 is larger than 0.11 because 0.9 is closer to 1. So even though both numbers have the same whole number part, the decimal part of 9.9 is bigger. Therefore, 9.9 should be larger than 9.11.\n",
       "\n",
       "But let me double-check. Maybe I should compare them digit by digit. Let's write them out:\n",
       "\n",
       "9.9 is the same as 9.90. So comparing 9.90 and 9.11. The first decimal place is 9 vs 1. Since 9 is greater than 1, the first number is larger. So yes, 9.9 is bigger than 9.11.\n",
       "\n",
       "I think that's right. There's no need to consider the whole numbers because they're the same. So the answer is 9.9.\n",
       "</think>\n",
       "\n",
       "9.9는 9.11보다 더 큰 수입니다. 두 수의 정수 부분은 모두 9입니다. 소수 부분을 비교하면, 9.9는 0.9, 9.11은 0.11입니다. 0.9는 0.11보다 크므로, 9.9가 더 큰 수입니다.\n",
       "\n",
       "**답변:** 9.9가 더 큰 수입니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6846a1",
   "metadata": {},
   "source": [
    "### LangGraph 를 사용하여 두개의 모델 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "053dc945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.7\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\\n\\n        당신의 작업은 다음과 같습니다:\\n        - 질문과 제공된 추론을 신중하게 분석하세요.\\n        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\\n        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\\n        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\\n\\n        지침:\\n        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\\n        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\\n        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\\n        - 도움이 되고 전문적인 톤을 유지하세요.\\n\\n        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        질문: {question}\\n        추론: {thinking}\\n        '), additional_kwargs={})]\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is bigger than 9.11. Hmm, both numbers are decimals, so I need to compare them step by step.\n",
      "\n",
      "First, I notice that both numbers start with 9 in the units place. That's the same, so I move to the tenths place. The first number, 9.9, has a 9 in the tenths place. The second number, 9.11, has a 1 in the tenths place. Wait, so 9 is larger than 1 in the tenths place. That means 9.9 is bigger than 9.11. But wait, maybe I should check if there's a decimal part to consider?\n",
      "\n",
      "Oh, right! 9.9 is the same as 9.90 when considering two decimal places. So comparing 9.90 and 9.11, the tenths place is 9 vs 1. Since 9 is greater, 9.90 is larger. So yes, 9.9 is bigger. I should make sure not to confuse the decimal places. Maybe the user is testing if they know that even though 9.11 has two decimal places, the tenths place is where the comparison happens. Yeah, that's right. So the key is the tenths place. The answer is 9.9 is larger.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 자연수 9을 공유하나, 소数 부분에서 비교해야 합니다.  \n",
      "9.9는 9.90과 같으며, 9.11은 9.11로 비교할 때, 소수점 첫 번째 자리에서 9가 1보다 더 큥다. 따라서 9.9는 9.11보다 더 큰 수입니다.\n",
      "{'question': '9.9와 9.11 중 무엇이 더 큰가요?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit from left to right:\\n\\n- The units place for both numbers is 9.\\n- In the tenths place, 9 has a 9 and 9.11 has a 1.\\n  \\nSince 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': \"<think>\\nOkay, let's tackle this question. The user is asking whether 9.9 is bigger than 9.11. Hmm, both numbers are decimals, so I need to compare them step by step.\\n\\nFirst, I notice that both numbers start with 9 in the units place. That's the same, so I move to the tenths place. The first number, 9.9, has a 9 in the tenths place. The second number, 9.11, has a 1 in the tenths place. Wait, so 9 is larger than 1 in the tenths place. That means 9.9 is bigger than 9.11. But wait, maybe I should check if there's a decimal part to consider?\\n\\nOh, right! 9.9 is the same as 9.90 when considering two decimal places. So comparing 9.90 and 9.11, the tenths place is 9 vs 1. Since 9 is greater, 9.90 is larger. So yes, 9.9 is bigger. I should make sure not to confuse the decimal places. Maybe the user is testing if they know that even though 9.11 has two decimal places, the tenths place is where the comparison happens. Yeah, that's right. So the key is the tenths place. The answer is 9.9 is larger.\\n</think>\\n\\n9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \\n두 수는 자연수 9을 공유하나, 소数 부분에서 비교해야 합니다.  \\n9.9는 9.90과 같으며, 9.11은 9.11로 비교할 때, 소수점 첫 번째 자리에서 9가 1보다 더 큥다. 따라서 9.9는 9.11보다 더 큰 수입니다.\"}\n",
      "==> 생성된 답변: \n",
      "\n",
      "<think>\n",
      "Okay, let's tackle this question. The user is asking whether 9.9 is bigger than 9.11. Hmm, both numbers are decimals, so I need to compare them step by step.\n",
      "\n",
      "First, I notice that both numbers start with 9 in the units place. That's the same, so I move to the tenths place. The first number, 9.9, has a 9 in the tenths place. The second number, 9.11, has a 1 in the tenths place. Wait, so 9 is larger than 1 in the tenths place. That means 9.9 is bigger than 9.11. But wait, maybe I should check if there's a decimal part to consider?\n",
      "\n",
      "Oh, right! 9.9 is the same as 9.90 when considering two decimal places. So comparing 9.90 and 9.11, the tenths place is 9 vs 1. Since 9 is greater, 9.90 is larger. So yes, 9.9 is bigger. I should make sure not to confuse the decimal places. Maybe the user is testing if they know that even though 9.11 has two decimal places, the tenths place is where the comparison happens. Yeah, that's right. So the key is the tenths place. The answer is 9.9 is larger.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다.  \n",
      "두 수는 자연수 9을 공유하나, 소数 부분에서 비교해야 합니다.  \n",
      "9.9는 9.90과 같으며, 9.11은 9.11로 비교할 때, 소수점 첫 번째 자리에서 9가 1보다 더 큥다. 따라서 9.9는 9.11보다 더 큰 수입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.7)\n",
    "print(generation_model)\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        당신은 사용자의 질문에 대해 명확하고 포괄적인 답변을 제공하는 AI 어시스턴트입니다.\n",
    "\n",
    "        당신의 작업은 다음과 같습니다:\n",
    "        - 질문과 제공된 추론을 신중하게 분석하세요.\n",
    "        - 추론에서 얻은 통찰력을 포함하여 잘 구조화된 답변을 생성하세요.\n",
    "        - 답변이 사용자의 질문에 직접적으로 대응하도록 하세요.\n",
    "        - 정보를 명확하고 자연스럽게 전달하되, 추론 과정을 명시적으로 언급하지 마세요.\n",
    "\n",
    "        지침:\n",
    "        - 답변을 대화 형식으로 작성하고, 흥미롭게 전달하세요.\n",
    "        - 중요한 포인트를 모두 다루면서도 명확하고 간결하게 작성하세요.\n",
    "        - 제공된 추론을 사용한다는 것을 언급하지 말고, 그 통찰력을 자연스럽게 포함시키세요.\n",
    "        - 도움이 되고 전문적인 톤을 유지하세요.\n",
    "\n",
    "        목표: 사용자의 질문에 직접적으로 대응하면서 추론 과정에서 얻은 통찰력을 자연스럽게 포함한 정보 제공입니다.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        질문: {question}\n",
    "        추론: {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n",
    "\n",
    "#LangGraph에서 State 사용자정의 클래스는 노드 간의 정보를 전달하는 틀입니다. \n",
    "#노드 간에 계속 전달하고 싶거나, 그래프 내에서 유지해야 할 정보를 미리 정의힙니다. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "#DeepSeek를 통해서 추론 부분까지만 생성합니다. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwen를 통해서 결과 출력 부분을 생성합니다.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# 입력 데이터\n",
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "# invoke()를 사용하여 그래프 호출\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"==> 생성된 답변: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0a890e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a9f22fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places. This means converting 9.9 into 9.90.\n",
      "\n",
      "Now that both numbers have two decimal places, I can directly compare them digit by digit from left to right.\n",
      "\n",
      "Starting with the units place: Both numbers have a 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, looking at the tenths place: The first number has a 9, and the second number has a 1. Since 9 is greater than 1, this means that 9.90 is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is greater than 9.11.\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure this out. Both numbers are decimals, so I should compare them digit by digit.\n",
      "\n",
      "First, I'll write them out: 9.9 and 9.11. Both have two decimal places, so maybe converting them to the same length would help. Wait, 9.9 is the same as 9.90. So, comparing 9.90 and 9.11.\n",
      "\n",
      "Starting from the left, the first digit is 9 in both. Then the next digit is 9 versus 1. Since 9 is greater than 1, 9.90 is larger. So 9.9 is bigger than 9.11. Got it. I should make sure there's no mistake here. Both numbers have the same integer part, so the difference is in the decimal part. The first number has a 9 in the tenths place, while the second has a 1. Yep, that's why 9.9 is larger. Alright, that's the reasoning.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다. 두 수 모두 정수 부분(9)과 소수 부분(0.9와 0.11)을 가집니다. 소수 부분을 비교할 때, 0.9는 0.11보다 크므로, 9.9가 더 커집니다. 이는 두 수의 정수 부분이 동일하고, 소수 부분에서 9가 1보다 큰 이유입니다.<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm, I need to figure this out. Both numbers are decimals, so I should compare them digit by digit.\n",
      "\n",
      "First, I'll write them out: 9.9 and 9.11. Both have two decimal places, so maybe converting them to the same length would help. Wait, 9.9 is the same as 9.90. So, comparing 9.90 and 9.11.\n",
      "\n",
      "Starting from the left, the first digit is 9 in both. Then the next digit is 9 versus 1. Since 9 is greater than 1, 9.90 is larger. So 9.9 is bigger than 9.11. Got it. I should make sure there's no mistake here. Both numbers have the same integer part, so the difference is in the decimal part. The first number has a 9 in the tenths place, while the second has a 1. Yep, that's why 9.9 is larger. Alright, that's the reasoning.\n",
      "</think>\n",
      "\n",
      "9.9와 9.11 중 더 큰 수는 **9.9**입니다. 두 수 모두 정수 부분(9)과 소수 부분(0.9와 0.11)을 가집니다. 소수 부분을 비교할 때, 0.9는 0.11보다 크므로, 9.9가 더 커집니다. 이는 두 수의 정수 부분이 동일하고, 소수 부분에서 9가 1보다 큰 이유입니다.\n"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"9.9와 9.11 중 무엇이 더 큰가요?\"}\n",
    "\n",
    "async for event in graph.astream_events(inputs, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(event['data']['chunk'].content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d254b0",
   "metadata": {},
   "source": [
    "### 2개의 모델을 연동한 코드를 Gradio를 사용하여 UI로 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4401293a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0a9bdad",
   "metadata": {},
   "source": [
    "### LangGraph 기본 예제: 두 개의 AI 에이전트 협력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
